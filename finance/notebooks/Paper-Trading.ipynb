{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ldTWXw3-e1h_Gmoj5VFJpjjSafXYRrPC","timestamp":1671549794165}],"mount_file_id":"10Z2PSZ635DPLvI5CkBhgonwk7-i4Alab","authorship_tag":"ABX9TyOVLPNx+JWj8NEolgvAbmzl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!git clone https://github.com/AI4Finance-Foundation/FinRL.git\n","!pip install swig\n","!pip install wrds\n","%pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CqV0PnOUN3uc","executionInfo":{"status":"ok","timestamp":1671551233991,"user_tz":480,"elapsed":5658,"user":{"displayName":"graham broughton","userId":"15728648374086258761"}},"outputId":"7b0cb04a-0566-4ffd-cf6d-14c08ec32eae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'FinRL'...\n","remote: Enumerating objects: 11696, done.\u001b[K\n","remote: Counting objects: 100% (130/130), done.\u001b[K\n","remote: Compressing objects: 100% (73/73), done.\u001b[K\n","remote: Total 11696 (delta 76), reused 102 (delta 55), pack-reused 11566\u001b[K\n","Receiving objects: 100% (11696/11696), 76.68 MiB | 24.45 MiB/s, done.\n","Resolving deltas: 100% (7709/7709), done.\n"]}]},{"cell_type":"code","source":["API_KEY = 'PKAVF23RLVWN3CI34P2Y'\n","API_SECRET = 'lr7FybhBKKkDiDVtxOpuI5McU8FZcaUnlYaBzOsp'\n","API_BASE_URL = 'https://paper-api.alpaca.markets'\n","data_url = 'wss://data.alpaca.markets'"],"metadata":{"id":"bZLXEElt39P_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from finrl.config_tickers import DOW_30_TICKER\n","from finrl.config import INDICATORS\n","from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n","from finrl.meta.env_stock_trading.env_stock_papertrading import AlpacaPaperTrading\n","from finrl.meta.data_processor import DataProcessor\n","from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n","\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"QMrML5kA2tC4","executionInfo":{"status":"ok","timestamp":1671551697699,"user_tz":480,"elapsed":6553,"user":{"displayName":"graham broughton","userId":"15728648374086258761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d941fd20-b089-4036-e892-aba8d6e6d05d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; multipliers will not be applied to position notionals.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","import gym\n","import numpy as np\n","import numpy.random as rd\n","import torch\n","import torch.nn as nn\n","from copy import deepcopy\n","from torch import Tensor\n","from torch.distributions.normal import Normal\n","\n","\n","class ActorPPO(nn.Module):\n","    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n","        super().__init__()\n","        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n","        self.action_std_log = nn.Parameter(torch.zeros((1, action_dim)), requires_grad=True)  # trainable parameter\n","\n","    def forward(self, state: Tensor) -> Tensor:\n","        return self.net(state).tanh()  # action.tanh()\n","\n","    def get_action(self, state: Tensor) -> (Tensor, Tensor):  # for exploration\n","        action_avg = self.net(state)\n","        action_std = self.action_std_log.exp()\n","\n","        dist = Normal(action_avg, action_std)\n","        action = dist.sample()\n","        logprob = dist.log_prob(action).sum(1)\n","        return action, logprob\n","\n","    def get_logprob_entropy(self, state: Tensor, action: Tensor) -> (Tensor, Tensor):\n","        action_avg = self.net(state)\n","        action_std = self.action_std_log.exp()\n","\n","        dist = Normal(action_avg, action_std)\n","        logprob = dist.log_prob(action).sum(1)\n","        entropy = dist.entropy().sum(1)\n","        return logprob, entropy\n","\n","    @staticmethod\n","    def convert_action_for_env(action: Tensor) -> Tensor:\n","        return action.tanh()\n","\n","\n","class CriticPPO(nn.Module):\n","    def __init__(self, dims: [int], state_dim: int, _action_dim: int):\n","        super().__init__()\n","        self.net = build_mlp(dims=[state_dim, *dims, 1])\n","\n","    def forward(self, state: Tensor) -> Tensor:\n","        return self.net(state)  # advantage value\n","\n","\n","def build_mlp(dims: [int]) -> nn.Sequential:  # MLP (MultiLayer Perceptron)\n","    net_list = []\n","    for i in range(len(dims) - 1):\n","        net_list.extend([nn.Linear(dims[i], dims[i + 1]), nn.ReLU()])\n","    del net_list[-1]  # remove the activation of output layer\n","    return nn.Sequential(*net_list)\n","\n","\n","class Config:\n","    def __init__(self, agent_class=None, env_class=None, env_args=None):\n","        self.env_class = env_class  # env = env_class(**env_args)\n","        self.env_args = env_args  # env = env_class(**env_args)\n","\n","        if env_args is None:  # dummy env_args\n","            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n","        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n","        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n","        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n","        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n","\n","        self.agent_class = agent_class  # agent = agent_class(...)\n","\n","        '''Arguments for reward shaping'''\n","        self.gamma = 0.99  # discount factor of future rewards\n","        self.reward_scale = 1.0  # an approximate target reward usually be closed to 256\n","\n","        '''Arguments for training'''\n","        self.gpu_id = int(0)  # `int` means the ID of single GPU, -1 means CPU\n","        self.net_dims = (64, 32)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n","        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n","        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n","        self.batch_size = int(128)  # num of transitions sampled from replay buffer.\n","        self.horizon_len = int(2000)  # collect horizon_len step while exploring, then update network\n","        self.buffer_size = None  # ReplayBuffer size. Empty the ReplayBuffer for on-policy.\n","        self.repeat_times = 8.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n","\n","        '''Arguments for evaluate'''\n","        self.cwd = None  # current working directory to save model. None means set automatically\n","        self.break_step = +np.inf  # break training if 'total_step > break_step'\n","        self.eval_times = int(32)  # number of times that get episodic cumulative return\n","        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n","\n","    def init_before_training(self):\n","        if self.cwd is None:  # set cwd (current working directory) for saving model\n","            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}'\n","        os.makedirs(self.cwd, exist_ok=True)\n","\n","\n","def get_gym_env_args(env, if_print: bool) -> dict:\n","    if {'unwrapped', 'observation_space', 'action_space', 'spec'}.issubset(dir(env)):  # isinstance(env, gym.Env):\n","        env_name = env.unwrapped.spec.id\n","        state_shape = env.observation_space.shape\n","        state_dim = state_shape[0] if len(state_shape) == 1 else state_shape  # sometimes state_dim is a list\n","\n","        if_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n","        if if_discrete:  # make sure it is discrete action space\n","            action_dim = env.action_space.n\n","        elif isinstance(env.action_space, gym.spaces.Box):  # make sure it is continuous action space\n","            action_dim = env.action_space.shape[0]\n","\n","    env_args = {'env_name': env_name, 'state_dim': state_dim, 'action_dim': action_dim, 'if_discrete': if_discrete}\n","    print(f\"env_args = {repr(env_args)}\") if if_print else None\n","    return env_args\n","\n","\n","def kwargs_filter(function, kwargs: dict) -> dict:\n","    import inspect\n","    sign = inspect.signature(function).parameters.values()\n","    sign = {val.name for val in sign}\n","    common_args = sign.intersection(kwargs.keys())\n","    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n","\n","\n","def build_env(env_class=None, env_args=None):\n","    if env_class.__module__ == 'gym.envs.registration':  # special rule\n","        env = env_class(id=env_args['env_name'])\n","    else:\n","        env = env_class(**kwargs_filter(env_class.__init__, env_args.copy()))\n","    for attr_str in ('env_name', 'state_dim', 'action_dim', 'if_discrete'):\n","        setattr(env, attr_str, env_args[attr_str])\n","    return env\n","\n","\n","class AgentBase:\n","    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","\n","        self.gamma = args.gamma\n","        self.batch_size = args.batch_size\n","        self.repeat_times = args.repeat_times\n","        self.reward_scale = args.reward_scale\n","        self.soft_update_tau = args.soft_update_tau\n","\n","        self.states = None  # assert self.states == (1, state_dim)\n","        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n","\n","        act_class = getattr(self, \"act_class\", None)\n","        cri_class = getattr(self, \"cri_class\", None)\n","        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n","        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n","            if cri_class else self.act\n","\n","        self.act_optimizer = torch.optim.Adam(self.act.parameters(), args.learning_rate)\n","        self.cri_optimizer = torch.optim.Adam(self.cri.parameters(), args.learning_rate) \\\n","            if cri_class else self.act_optimizer\n","\n","        self.criterion = torch.nn.SmoothL1Loss()\n","\n","    @staticmethod\n","    def optimizer_update(optimizer, objective: Tensor):\n","        optimizer.zero_grad()\n","        objective.backward()\n","        optimizer.step()\n","\n","    @staticmethod\n","    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n","        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n","            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n","\n","\n","class AgentPPO(AgentBase):\n","    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n","        self.if_off_policy = False\n","        self.act_class = getattr(self, \"act_class\", ActorPPO)\n","        self.cri_class = getattr(self, \"cri_class\", CriticPPO)\n","        AgentBase.__init__(self, net_dims, state_dim, action_dim, gpu_id, args)\n","\n","        self.ratio_clip = getattr(args, \"ratio_clip\", 0.25)  # `ratio.clamp(1 - clip, 1 + clip)`\n","        self.lambda_gae_adv = getattr(args, \"lambda_gae_adv\", 0.95)  # could be 0.80~0.99\n","        self.lambda_entropy = getattr(args, \"lambda_entropy\", 0.01)  # could be 0.00~0.10\n","        self.lambda_entropy = torch.tensor(self.lambda_entropy, dtype=torch.float32, device=self.device)\n","\n","    def explore_env(self, env, horizon_len: int) -> [Tensor]:\n","        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n","        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n","        logprobs = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n","        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n","        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n","\n","        ary_state = self.states[0]\n","\n","        get_action = self.act.get_action\n","        convert = self.act.convert_action_for_env\n","        for i in range(horizon_len):\n","            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n","            action, logprob = [t.squeeze(0) for t in get_action(state.unsqueeze(0))[:2]]\n","\n","            ary_action = convert(action).detach().cpu().numpy()\n","            ary_state, reward, done, _ = env.step(ary_action)\n","            if done:\n","                ary_state = env.reset()\n","\n","            states[i] = state\n","            actions[i] = action\n","            logprobs[i] = logprob\n","            rewards[i] = reward\n","            dones[i] = done\n","\n","        self.states[0] = ary_state\n","        rewards = (rewards * self.reward_scale).unsqueeze(1)\n","        undones = (1 - dones.type(torch.float32)).unsqueeze(1)\n","        return states, actions, logprobs, rewards, undones\n","\n","    def update_net(self, buffer) -> [float]:\n","        with torch.no_grad():\n","            states, actions, logprobs, rewards, undones = buffer\n","            buffer_size = states.shape[0]\n","\n","            '''get advantages reward_sums'''\n","            bs = 2 ** 10  # set a smaller 'batch_size' when out of GPU memory.\n","            values = [self.cri(states[i:i + bs]) for i in range(0, buffer_size, bs)]\n","            values = torch.cat(values, dim=0).squeeze(1)  # values.shape == (buffer_size, )\n","\n","            advantages = self.get_advantages(rewards, undones, values)  # advantages.shape == (buffer_size, )\n","            reward_sums = advantages + values  # reward_sums.shape == (buffer_size, )\n","            del rewards, undones, values\n","\n","            advantages = (advantages - advantages.mean()) / (advantages.std(dim=0) + 1e-5)\n","        assert logprobs.shape == advantages.shape == reward_sums.shape == (buffer_size,)\n","\n","        '''update network'''\n","        obj_critics = 0.0\n","        obj_actors = 0.0\n","\n","        update_times = int(buffer_size * self.repeat_times / self.batch_size)\n","        assert update_times >= 1\n","        for _ in range(update_times):\n","            indices = torch.randint(buffer_size, size=(self.batch_size,), requires_grad=False)\n","            state = states[indices]\n","            action = actions[indices]\n","            logprob = logprobs[indices]\n","            advantage = advantages[indices]\n","            reward_sum = reward_sums[indices]\n","\n","            value = self.cri(state).squeeze(1)  # critic network predicts the reward_sum (Q value) of state\n","            obj_critic = self.criterion(value, reward_sum)\n","            self.optimizer_update(self.cri_optimizer, obj_critic)\n","\n","            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)\n","            ratio = (new_logprob - logprob.detach()).exp()\n","            surrogate1 = advantage * ratio\n","            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)\n","            obj_surrogate = torch.min(surrogate1, surrogate2).mean()\n","\n","            obj_actor = obj_surrogate + obj_entropy.mean() * self.lambda_entropy\n","            self.optimizer_update(self.act_optimizer, -obj_actor)\n","\n","            obj_critics += obj_critic.item()\n","            obj_actors += obj_actor.item()\n","        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1)).mean()\n","        return obj_critics / update_times, obj_actors / update_times, a_std_log.item()\n","\n","    def get_advantages(self, rewards: Tensor, undones: Tensor, values: Tensor) -> Tensor:\n","        advantages = torch.empty_like(values)  # advantage value\n","\n","        masks = undones * self.gamma\n","        horizon_len = rewards.shape[0]\n","\n","        next_state = torch.tensor(self.states, dtype=torch.float32).to(self.device)\n","        next_value = self.cri(next_state).detach()[0, 0]\n","\n","        advantage = 0  # last_gae_lambda\n","        for t in range(horizon_len - 1, -1, -1):\n","            delta = rewards[t] + masks[t] * next_value - values[t]\n","            advantages[t] = advantage = delta + masks[t] * self.lambda_gae_adv * advantage\n","            next_value = values[t]\n","        return advantages\n","\n","\n","class PendulumEnv(gym.Wrapper):  # a demo of custom gym env\n","    def __init__(self):\n","        gym.logger.set_level(40)  # Block warning\n","        gym_env_name = \"Pendulum-v0\" if gym.__version__ < '0.18.0' else \"Pendulum-v1\"\n","        super().__init__(env=gym.make(gym_env_name))\n","\n","        '''the necessary env information when you design a custom env'''\n","        self.env_name = gym_env_name  # the name of this env.\n","        self.state_dim = self.observation_space.shape[0]  # feature number of state\n","        self.action_dim = self.action_space.shape[0]  # feature number of action\n","        self.if_discrete = False  # discrete action or continuous action\n","\n","    def reset(self) -> np.ndarray:  # reset the agent in env\n","        return self.env.reset()\n","\n","    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):  # agent interacts in env\n","        # We suggest that adjust action space to (-1, +1) when designing a custom env.\n","        state, reward, done, info_dict = self.env.step(action * 2)\n","        return state.reshape(self.state_dim), float(reward), done, info_dict\n","\n","    \n","def train_agent(args: Config):\n","    args.init_before_training()\n","\n","    env = build_env(args.env_class, args.env_args)\n","    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n","    agent.states = env.reset()[np.newaxis, :]\n","\n","    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n","                          eval_per_step=args.eval_per_step,\n","                          eval_times=args.eval_times,\n","                          cwd=args.cwd)\n","    torch.set_grad_enabled(False)\n","    while True: # start training\n","        buffer_items = agent.explore_env(env, args.horizon_len)\n","\n","        torch.set_grad_enabled(True)\n","        logging_tuple = agent.update_net(buffer_items)\n","        torch.set_grad_enabled(False)\n","\n","        evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n","        if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n","            torch.save(agent.act.state_dict(), args.cwd + '/actor.pth')\n","            break  # stop training when reach `break_step` or `mkdir cwd/stop`\n","\n","\n","def render_agent(env_class, env_args: dict, net_dims: [int], agent_class, actor_path: str, render_times: int = 8):\n","    env = build_env(env_class, env_args)\n","\n","    state_dim = env_args['state_dim']\n","    action_dim = env_args['action_dim']\n","    agent = agent_class(net_dims, state_dim, action_dim, gpu_id=-1)\n","    actor = agent.act\n","\n","    print(f\"| render and load actor from: {actor_path}\")\n","    actor.load_state_dict(torch.load(actor_path, map_location=lambda storage, loc: storage))\n","    for i in range(render_times):\n","        cumulative_reward, episode_step = get_rewards_and_steps(env, actor, if_render=True)\n","        print(f\"|{i:4}  cumulative_reward {cumulative_reward:9.3f}  episode_step {episode_step:5.0f}\")\n","\n","        \n","class Evaluator:\n","    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.'):\n","        self.cwd = cwd\n","        self.env_eval = eval_env\n","        self.eval_step = 0\n","        self.total_step = 0\n","        self.start_time = time.time()\n","        self.eval_times = eval_times  # number of times that get episodic cumulative return\n","        self.eval_per_step = eval_per_step  # evaluate the agent per training steps\n","\n","        self.recorder = []\n","        print(f\"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n","              f\"\\n| `time`: Time spent from the start of training to this moment.\"\n","              f\"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n","              f\"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n","              f\"\\n| `avgS`: Average of steps in an episode.\"\n","              f\"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n","              f\"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n","              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n","            \n","    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n","        self.total_step += horizon_len\n","        if self.eval_step + self.eval_per_step > self.total_step:\n","            return\n","        self.eval_step = self.total_step\n","\n","        rewards_steps_ary = [get_rewards_and_steps(self.env_eval, actor) for _ in range(self.eval_times)]\n","        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n","        avg_r = rewards_steps_ary[:, 0].mean()  # average of cumulative rewards\n","        std_r = rewards_steps_ary[:, 0].std()  # std of cumulative rewards\n","        avg_s = rewards_steps_ary[:, 1].mean()  # average of steps in an episode\n","\n","        used_time = time.time() - self.start_time\n","        self.recorder.append((self.total_step, used_time, avg_r))\n","        \n","        print(f\"| {self.total_step:8.2e}  {used_time:8.0f}  \"\n","              f\"| {avg_r:8.2f}  {std_r:6.2f}  {avg_s:6.0f}  \"\n","              f\"| {logging_tuple[0]:8.2f}  {logging_tuple[1]:8.2f}\")\n","\n","\n","def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n","    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n","\n","    state = env.reset()\n","    episode_steps = 0\n","    cumulative_returns = 0.0  # sum of rewards in an episode\n","    for episode_steps in range(12345):\n","        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","        tensor_action = actor(tensor_state)\n","        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n","        state, reward, done, _ = env.step(action)\n","        cumulative_returns += reward\n","\n","        if if_render:\n","            env.render()\n","        if done:\n","            break\n","    return cumulative_returns, episode_steps + 1"],"metadata":{"id":"704dSyHE6VTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import annotations\n","\n","import torch\n","# from elegantrl.agents import AgentA2C\n","\n","MODELS = {\"ppo\": AgentPPO}\n","OFF_POLICY_MODELS = [\"ddpg\", \"td3\", \"sac\"]\n","ON_POLICY_MODELS = [\"ppo\"]\n","# MODEL_KWARGS = {x: config.__dict__[f\"{x.upper()}_PARAMS\"] for x in MODELS.keys()}\n","#\n","# NOISE = {\n","#     \"normal\": NormalActionNoise,\n","#     \"ornstein_uhlenbeck\": OrnsteinUhlenbeckActionNoise,\n","# }\n","\n","\n","class DRLAgent:\n","    \"\"\"Implementations of DRL algorithms\n","    Attributes\n","    ----------\n","        env: gym environment class\n","            user-defined class\n","    Methods\n","    -------\n","        get_model()\n","            setup DRL algorithms\n","        train_model()\n","            train DRL algorithms in a train dataset\n","            and output the trained model\n","        DRL_prediction()\n","            make a prediction in a test dataset and get results\n","    \"\"\"\n","\n","    def __init__(self, env, price_array, tech_array, turbulence_array):\n","        self.env = env\n","        self.price_array = price_array\n","        self.tech_array = tech_array\n","        self.turbulence_array = turbulence_array\n","\n","    def get_model(self, model_name, model_kwargs):\n","        env_config = {\n","            \"price_array\": self.price_array,\n","            \"tech_array\": self.tech_array,\n","            \"turbulence_array\": self.turbulence_array,\n","            \"if_train\": True,\n","        }\n","        environment = self.env(config=env_config)\n","        env_args = {'config': env_config,\n","              'env_name': environment.env_name,\n","              'state_dim': environment.state_dim,\n","              'action_dim': environment.action_dim,\n","              'if_discrete': False}\n","        agent = MODELS[model_name]\n","        if model_name not in MODELS:\n","            raise NotImplementedError(\"NotImplementedError\")\n","        model = Config(agent_class=agent, env_class=self.env, env_args=env_args)\n","        model.if_off_policy = model_name in OFF_POLICY_MODELS\n","        if model_kwargs is not None:\n","            try:\n","                model.learning_rate = model_kwargs[\"learning_rate\"]\n","                model.batch_size = model_kwargs[\"batch_size\"]\n","                model.gamma = model_kwargs[\"gamma\"]\n","                model.seed = model_kwargs[\"seed\"]\n","                model.net_dims = model_kwargs[\"net_dimension\"]\n","                model.target_step = model_kwargs[\"target_step\"]\n","                model.eval_gap = model_kwargs[\"eval_gap\"]\n","                model.eval_times = model_kwargs[\"eval_times\"]\n","            except BaseException:\n","                raise ValueError(\n","                    \"Fail to read arguments, please check 'model_kwargs' input.\"\n","                )\n","        return model\n","\n","    def train_model(self, model, cwd, total_timesteps=5000):\n","        model.cwd = cwd\n","        model.break_step = total_timesteps\n","        train_agent(model)\n","\n","    @staticmethod\n","    def DRL_prediction(model_name, cwd, net_dimension, environment):\n","        if model_name not in MODELS:\n","            raise NotImplementedError(\"NotImplementedError\")\n","        agent_class = MODELS[model_name]\n","        environment.env_num = 1\n","        agent = agent_class(net_dimension, environment.state_dim, environment.action_dim)\n","        actor = agent.act\n","        # load agent\n","        try:  \n","            cwd = cwd + '/actor.pth'\n","            print(f\"| load actor from: {cwd}\")\n","            actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n","            act = actor\n","            device = agent.device\n","        except BaseException:\n","            raise ValueError(\"Fail to load agent!\")\n","\n","        # test on the testing env\n","        _torch = torch\n","        state = environment.reset()\n","        episode_returns = []  # the cumulative_return / initial_account\n","        episode_total_assets = [environment.initial_total_asset]\n","        with _torch.no_grad():\n","            for i in range(environment.max_step):\n","                s_tensor = _torch.as_tensor((state,), device=device)\n","                a_tensor = act(s_tensor)  # action_tanh = act.forward()\n","                action = (\n","                    a_tensor.detach().cpu().numpy()[0]\n","                )  # not need detach(), because with torch.no_grad() outside\n","                state, reward, done, _ = environment.step(action)\n","\n","                total_asset = (\n","                    environment.amount\n","                    + (\n","                        environment.price_ary[environment.day] * environment.stocks\n","                    ).sum()\n","                )\n","                episode_total_assets.append(total_asset)\n","                episode_return = total_asset / environment.initial_total_asset\n","                episode_returns.append(episode_return)\n","                if done:\n","                    break\n","        print(\"Test Finished!\")\n","        # return episode total_assets on testing data\n","        print(\"episode_return\", episode_return)\n","        return episode_total_assets"],"metadata":{"id":"OVehCXKU3S3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import annotations\n","\n","from finrl.config import ERL_PARAMS\n","from finrl.config import INDICATORS\n","from finrl.config import RLlib_PARAMS\n","from finrl.config import SAC_PARAMS\n","from finrl.config import TRAIN_END_DATE\n","from finrl.config import TRAIN_START_DATE\n","from finrl.config_tickers import DOW_30_TICKER\n","from finrl.meta.data_processor import DataProcessor\n","\n","# construct environment\n","\n","\n","def train(\n","    start_date,\n","    end_date,\n","    ticker_list,\n","    data_source,\n","    time_interval,\n","    technical_indicator_list,\n","    drl_lib,\n","    env,\n","    model_name,\n","    if_vix=True,\n","    **kwargs,\n","):\n","    # download data\n","    dp = DataProcessor(data_source, **kwargs)\n","    data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n","    data = dp.clean_data(data)\n","    data = dp.add_technical_indicator(data, technical_indicator_list)\n","    if if_vix:\n","        data = dp.add_vix(data)\n","    else:\n","        data = dp.add_turbulence(data)\n","    price_array, tech_array, turbulence_array = dp.df_to_array(data, if_vix)\n","    env_config = {\n","        \"price_array\": price_array,\n","        \"tech_array\": tech_array,\n","        \"turbulence_array\": turbulence_array,\n","        \"if_train\": True,\n","    }\n","    env_instance = env(config=env_config)\n","\n","    # read parameters\n","    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n","\n","    if drl_lib == \"elegantrl\":\n","        DRLAgent_erl = DRLAgent\n","        break_step = kwargs.get(\"break_step\", 1e6)\n","        erl_params = kwargs.get(\"erl_params\")\n","        agent = DRLAgent_erl(\n","            env=env,\n","            price_array=price_array,\n","            tech_array=tech_array,\n","            turbulence_array=turbulence_array,\n","        )\n","        model = agent.get_model(model_name, model_kwargs=erl_params)\n","        trained_model = agent.train_model(\n","            model=model, cwd=cwd, total_timesteps=break_step\n","        )"],"metadata":{"id":"Y5GYu83e3S0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import annotations\n","\n","from finrl.config import INDICATORS\n","from finrl.config import RLlib_PARAMS\n","from finrl.config import TEST_END_DATE\n","from finrl.config import TEST_START_DATE\n","from finrl.config_tickers import DOW_30_TICKER\n","\n","def test(\n","    start_date,\n","    end_date,\n","    ticker_list,\n","    data_source,\n","    time_interval,\n","    technical_indicator_list,\n","    drl_lib,\n","    env,\n","    model_name,\n","    if_vix=True,\n","    **kwargs,\n","):\n","\n","    # import data processor\n","    from finrl.meta.data_processor import DataProcessor\n","\n","    # fetch data\n","    dp = DataProcessor(data_source, **kwargs)\n","    data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n","    data = dp.clean_data(data)\n","    data = dp.add_technical_indicator(data, technical_indicator_list)\n","\n","    if if_vix:\n","        data = dp.add_vix(data)\n","    else:\n","        data = dp.add_turbulence(data)\n","    price_array, tech_array, turbulence_array = dp.df_to_array(data, if_vix)\n","\n","    env_config = {\n","        \"price_array\": price_array,\n","        \"tech_array\": tech_array,\n","        \"turbulence_array\": turbulence_array,\n","        \"if_train\": False,\n","    }\n","    env_instance = env(config=env_config)\n","\n","    # load elegantrl needs state dim, action dim and net dim\n","    net_dimension = kwargs.get(\"net_dimension\", 2**7)\n","    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n","    print(\"price_array: \", len(price_array))\n","\n","    if drl_lib == \"elegantrl\":\n","        DRLAgent_erl = DRLAgent\n","        episode_total_assets = DRLAgent_erl.DRL_prediction(\n","            model_name=model_name,\n","            cwd=cwd,\n","            net_dimension=net_dimension,\n","            environment=env_instance,\n","        )\n","        return episode_total_assets"],"metadata":{"id":"pKs7K1Zm3SyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ticker_list = DOW_30_TICKER\n","action_dim = len(DOW_30_TICKER)\n","state_dim = 1 + 2 + 3 * action_dim + len(INDICATORS) * action_dim\n","env = StockTradingEnv"],"metadata":{"id":"A2ERYqoY3Soz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ERL_PARAMS = {\"learning_rate\": 3e-6,\"batch_size\": 2048,\"gamma\":  0.985,\n","        \"seed\":312,\"net_dimension\":[128,64], \"target_step\":5000, \"eval_gap\":30,\n","        \"eval_times\":1} "],"metadata":{"id":"7aAeRmCD3SV2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","import threading\n","from finrl.meta.data_processors.processor_alpaca import AlpacaProcessor\n","import alpaca_trade_api as tradeapi\n","import time\n","import pandas as pd\n","import numpy as np\n","import torch\n","import gym\n","\n","class AlpacaPaperTrading():\n","\n","    def __init__(self,ticker_list, time_interval, drl_lib, agent, cwd, net_dim, \n","                 state_dim, action_dim, API_KEY, API_SECRET, \n","                 API_BASE_URL, tech_indicator_list, turbulence_thresh=30, \n","                 max_stock=1e2, latency = None):\n","        #load agent\n","        self.drl_lib = drl_lib\n","        if agent =='ppo':\n","            if drl_lib == 'elegantrl':              \n","                agent_class = AgentPPO\n","                agent = agent_class(net_dim, state_dim, action_dim)\n","                actor = agent.act\n","                # load agent\n","                try:  \n","                    cwd = cwd + '/actor.pth'\n","                    print(f\"| load actor from: {cwd}\")\n","                    actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n","                    self.act = actor\n","                    self.device = agent.device\n","                except BaseException:\n","                    raise ValueError(\"Fail to load agent!\")\n","                        \n","            elif drl_lib == 'rllib':\n","                from ray.rllib.agents import ppo\n","                from ray.rllib.agents.ppo.ppo import PPOTrainer\n","                \n","                config = ppo.DEFAULT_CONFIG.copy()\n","                config['env'] = StockEnvEmpty\n","                config[\"log_level\"] = \"WARN\"\n","                config['env_config'] = {'state_dim':state_dim,\n","                            'action_dim':action_dim,}\n","                trainer = PPOTrainer(env=StockEnvEmpty, config=config)\n","                trainer.restore(cwd)\n","                try:\n","                    trainer.restore(cwd)\n","                    self.agent = trainer\n","                    print(\"Restoring from checkpoint path\", cwd)\n","                except:\n","                    raise ValueError('Fail to load agent!')\n","                    \n","            elif drl_lib == 'stable_baselines3':\n","                from stable_baselines3 import PPO\n","                \n","                try:\n","                    #load agent\n","                    self.model = PPO.load(cwd)\n","                    print(\"Successfully load model\", cwd)\n","                except:\n","                    raise ValueError('Fail to load agent!')\n","                    \n","            else:\n","                raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n","               \n","        else:\n","            raise ValueError('Agent input is NOT supported yet.')\n","            \n","            \n","            \n","        #connect to Alpaca trading API\n","        try:\n","            self.alpaca = tradeapi.REST(API_KEY,API_SECRET,API_BASE_URL, 'v2')\n","        except:\n","            raise ValueError('Fail to connect Alpaca. Please check account info and internet connection.')\n","        \n","        #read trading time interval\n","        if time_interval == '1s':\n","            self.time_interval = 1\n","        elif time_interval == '5s':\n","            self.time_interval = 5\n","        elif time_interval == '1Min':\n","            self.time_interval = 60\n","        elif time_interval == '5Min':\n","            self.time_interval = 60 * 5\n","        elif time_interval == '15Min':\n","            self.time_interval = 60 * 15\n","        else:\n","            raise ValueError('Time interval input is NOT supported yet.')\n","        \n","        #read trading settings\n","        self.tech_indicator_list = tech_indicator_list\n","        self.turbulence_thresh = turbulence_thresh\n","        self.max_stock = max_stock \n","        \n","        #initialize account\n","        self.stocks = np.asarray([0] * len(ticker_list)) #stocks holding\n","        self.stocks_cd = np.zeros_like(self.stocks) \n","        self.cash = None #cash record \n","        self.stocks_df = pd.DataFrame(self.stocks, columns=['stocks'], index = ticker_list)\n","        self.asset_list = []\n","        self.price = np.asarray([0] * len(ticker_list))\n","        self.stockUniverse = ticker_list\n","        self.turbulence_bool = 0\n","        self.equities = []\n","        \n","    def test_latency(self, test_times = 10): \n","        total_time = 0\n","        for i in range(0, test_times):\n","            time0 = time.time()\n","            self.get_state()\n","            time1 = time.time()\n","            temp_time = time1 - time0\n","            total_time += temp_time\n","        latency = total_time/test_times\n","        print('latency for data processing: ', latency)\n","        return latency\n","        \n","    def run(self):\n","        orders = self.alpaca.list_orders(status=\"open\")\n","        for order in orders:\n","          self.alpaca.cancel_order(order.id)\n","    \n","        # Wait for market to open.\n","        print(\"Waiting for market to open...\")\n","        tAMO = threading.Thread(target=self.awaitMarketOpen)\n","        tAMO.start()\n","        tAMO.join()\n","        print(\"Market opened.\")\n","        while True:\n","\n","          # Figure out when the market will close so we can prepare to sell beforehand.\n","          clock = self.alpaca.get_clock()\n","          closingTime = clock.next_close.replace(tzinfo=datetime.timezone.utc).timestamp()\n","          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n","          self.timeToClose = closingTime - currTime\n","    \n","          if(self.timeToClose < (60)):\n","            # Close all positions when 1 minutes til market close.\n","            print(\"Market closing soon. Stop trading.\")\n","            break\n","            \n","            '''# Close all positions when 1 minutes til market close.\n","            print(\"Market closing soon.  Closing positions.\")\n","    \n","            positions = self.alpaca.list_positions()\n","            for position in positions:\n","              if(position.side == 'long'):\n","                orderSide = 'sell'\n","              else:\n","                orderSide = 'buy'\n","              qty = abs(int(float(position.qty)))\n","              respSO = []\n","              tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n","              tSubmitOrder.start()\n","              tSubmitOrder.join()\n","    \n","            # Run script again after market close for next trading day.\n","            print(\"Sleeping until market close (15 minutes).\")\n","            time.sleep(60 * 15)'''\n","            \n","          else:\n","            trade = threading.Thread(target=self.trade)\n","            trade.start()\n","            trade.join()\n","            last_equity = float(self.alpaca.get_account().last_equity)\n","            cur_time = time.time()\n","            self.equities.append([cur_time,last_equity])\n","            time.sleep(self.time_interval)\n","            \n","    def awaitMarketOpen(self):\n","        isOpen = self.alpaca.get_clock().is_open\n","        while(not isOpen):\n","          clock = self.alpaca.get_clock()\n","          openingTime = clock.next_open.replace(tzinfo=datetime.timezone.utc).timestamp()\n","          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n","          timeToOpen = int((openingTime - currTime) / 60)\n","          print(str(timeToOpen) + \" minutes til market open.\")\n","          time.sleep(60)\n","          isOpen = self.alpaca.get_clock().is_open\n","    \n","    def trade(self):\n","        state = self.get_state()\n","        \n","        if self.drl_lib == 'elegantrl':\n","            with torch.no_grad():\n","                s_tensor = torch.as_tensor((state,), device=self.device)\n","                a_tensor = self.act(s_tensor)  \n","                action = a_tensor.detach().cpu().numpy()[0]  \n","            action = (action * self.max_stock).astype(int)\n","            \n","        elif self.drl_lib == 'rllib':\n","            action = self.agent.compute_single_action(state)\n","        \n","        elif self.drl_lib == 'stable_baselines3':\n","            action = self.model.predict(state)[0]\n","            \n","        else:\n","            raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n","        \n","        self.stocks_cd += 1\n","        if self.turbulence_bool == 0:\n","            min_action = 10  # stock_cd\n","            for index in np.where(action < -min_action)[0]:  # sell_index:\n","                sell_num_shares = min(self.stocks[index], -action[index])\n","                qty =  abs(int(sell_num_shares))\n","                respSO = []\n","                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'sell', respSO))\n","                tSubmitOrder.start()\n","                tSubmitOrder.join()\n","                self.cash = float(self.alpaca.get_account().cash)\n","                self.stocks_cd[index] = 0\n","\n","            for index in np.where(action > min_action)[0]:  # buy_index:\n","                if self.cash < 0:\n","                    tmp_cash = 0\n","                else:\n","                    tmp_cash = self.cash\n","                buy_num_shares = min(tmp_cash // self.price[index], abs(int(action[index])))\n","                if (buy_num_shares != buy_num_shares): # if buy_num_change = nan\n","                    qty = 0 # set to 0 quantity\n","                else:\n","                    qty = abs(int(buy_num_shares))\n","                qty = abs(int(buy_num_shares))\n","                respSO = []\n","                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'buy', respSO))\n","                tSubmitOrder.start()\n","                tSubmitOrder.join()\n","                self.cash = float(self.alpaca.get_account().cash)\n","                self.stocks_cd[index] = 0\n","                \n","        else:  # sell all when turbulence\n","            positions = self.alpaca.list_positions()\n","            for position in positions:\n","                if(position.side == 'long'):\n","                    orderSide = 'sell'\n","                else:\n","                    orderSide = 'buy'\n","                qty = abs(int(float(position.qty)))\n","                respSO = []\n","                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n","                tSubmitOrder.start()\n","                tSubmitOrder.join()\n","            \n","            self.stocks_cd[:] = 0\n","            \n","    \n","    def get_state(self):\n","        alpaca = AlpacaProcessor(api=self.alpaca)\n","        price, tech, turbulence = alpaca.fetch_latest_data(ticker_list = self.stockUniverse, time_interval='1Min',\n","                                                     tech_indicator_list=self.tech_indicator_list)\n","        turbulence_bool = 1 if turbulence >= self.turbulence_thresh else 0\n","        \n","        turbulence = (self.sigmoid_sign(turbulence, self.turbulence_thresh) * 2 ** -5).astype(np.float32)\n","        \n","        tech = tech * 2 ** -7\n","        positions = self.alpaca.list_positions()\n","        stocks = [0] * len(self.stockUniverse)\n","        for position in positions:\n","            ind = self.stockUniverse.index(position.symbol)\n","            stocks[ind] = ( abs(int(float(position.qty))))\n","        \n","        stocks = np.asarray(stocks, dtype = float)\n","        cash = float(self.alpaca.get_account().cash)\n","        self.cash = cash\n","        self.stocks = stocks\n","        self.turbulence_bool = turbulence_bool \n","        self.price = price\n","        \n","        \n","        \n","        amount = np.array(self.cash * (2 ** -12), dtype=np.float32)\n","        scale = np.array(2 ** -6, dtype=np.float32)\n","        state = np.hstack((amount,\n","                    turbulence,\n","                    self.turbulence_bool,\n","                    price * scale,\n","                    self.stocks * scale,\n","                    self.stocks_cd,\n","                    tech,\n","                    )).astype(np.float32)\n","        state[np.isnan(state)] = 0.0\n","        state[np.isinf(state)] = 0.0\n","        print(len(self.stockUniverse))\n","        return state\n","        \n","    def submitOrder(self, qty, stock, side, resp):\n","        if(qty > 0):\n","          try:\n","            self.alpaca.submit_order(stock, qty, side, \"market\", \"day\")\n","            print(\"Market order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | completed.\")\n","            resp.append(True)\n","          except:\n","            print(\"Order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | did not go through.\")\n","            resp.append(False)\n","        else:\n","          print(\"Quantity is 0, order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | not completed.\")\n","          resp.append(True)\n","\n","    @staticmethod\n","    def sigmoid_sign(ary, thresh):\n","        def sigmoid(x):\n","            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n","\n","        return sigmoid(ary / thresh) * thresh\n","    \n","class StockEnvEmpty(gym.Env):\n","    #Empty Env used for loading rllib agent\n","    def __init__(self,config):\n","      state_dim = config['state_dim']\n","      action_dim = config['action_dim']\n","      self.env_num = 1\n","      self.max_step = 10000\n","      self.env_name = 'StockEnvEmpty'\n","      self.state_dim = state_dim  \n","      self.action_dim = action_dim\n","      self.if_discrete = False  \n","      self.target_return = 9999\n","      self.observation_space = gym.spaces.Box(low=-3000, high=3000, shape=(state_dim,), dtype=np.float32)\n","      self.action_space = gym.spaces.Box(low=-1, high=1, shape=(action_dim,), dtype=np.float32)\n","        \n","    def reset(self):\n","        return \n","\n","    def step(self, actions):\n","        return"],"metadata":{"id":"ET-d2_EL3STP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["paper_trading_erl = AlpacaPaperTrading(ticker_list = DOW_30_TICKER, \n","                                       time_interval = '1Min', \n","                                       drl_lib = 'elegantrl', \n","                                       agent = 'ppo', \n","                                       cwd = './papertrading_erl_retrain', \n","                                       net_dim = ERL_PARAMS['net_dimension'], \n","                                       state_dim = state_dim, \n","                                       action_dim= action_dim, \n","                                       API_KEY = API_KEY, \n","                                       API_SECRET = API_SECRET, \n","                                       API_BASE_URL = API_BASE_URL, \n","                                       tech_indicator_list = INDICATORS, \n","                                       turbulence_thresh=30, \n","                                       max_stock=20)\n","paper_trading_erl.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"id":"Yxj44k8WBv6E","outputId":"9cab9006-dc1e-427f-97ba-de1eea5410c9","executionInfo":{"status":"error","timestamp":1671551801876,"user_tz":480,"elapsed":101984,"user":{"displayName":"graham broughton","userId":"15728648374086258761"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["| load actor from: ./papertrading_erl_retrain/actor.pth\n","Waiting for market to open...\n","Market opened.\n","30\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-7a13ac00a537>:186: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n","  s_tensor = torch.as_tensor((state,), device=self.device)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5f03a65cfcbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                        \u001b[0mturbulence_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                        max_stock=20)\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpaper_trading_erl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-7a13ac00a537>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mtrade\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mtrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mlast_equity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpaca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_account\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_equity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mcur_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"IYG0oxfjCREm"},"execution_count":null,"outputs":[]}]}
